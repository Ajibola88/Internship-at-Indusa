{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use natural language toolkit\n",
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "stemmer = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data = []\n",
    "import pandas as pd\n",
    "data=pd.read_csv('C:/Users/trainees/Desktop/AllData/data/Classificationtable3.csv')\n",
    "data = data[pd.notnull(data['tokenized_source'])]\n",
    "data=data[data.Category != 'None']\n",
    "for index,row in data.iterrows():\n",
    "    training_data.append({\"class\":row[\"Category\"], \"sentence\":row[\"tokenized_source\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check number of statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1594 sentences of training data\n"
     ]
    }
   ],
   "source": [
    "print (\"%s sentences of training data\" % len(training_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividing data as array of unique stemmed words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1594 documents\n",
      "3 classes ['Class_1', 'Class_3', 'Class_2']\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_words = ['?']\n",
    "# loop through each sentence in our training data\n",
    "for pattern in training_data:\n",
    "    # tokenize each word in the sentence\n",
    "    w = nltk.word_tokenize(pattern['sentence'])\n",
    "    # add to our words list\n",
    "    words.extend(w)\n",
    "    # add to documents in our corpus\n",
    "    documents.append((w, pattern['class']))\n",
    "    # add to our classes list\n",
    "    if pattern['class'] not in classes:\n",
    "        classes.append(pattern['class'])\n",
    "\n",
    "# stem and lower each word and remove duplicates\n",
    "words = [stemmer.stem(w.lower()) for w in words if w not in ignore_words]\n",
    "words = list(set(words))\n",
    "\n",
    "# remove duplicates\n",
    "classes = list(set(classes))\n",
    "\n",
    "print (len(documents), \"documents\")\n",
    "print (len(classes), \"classes\", classes)\n",
    "#print (len(words), \"unique stemmed words\", words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "geting list of unique words and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# words 41468\n",
      "# classes 3\n"
     ]
    }
   ],
   "source": [
    "# create our training data\n",
    "training = []\n",
    "output = []\n",
    "# create an empty array for our output\n",
    "output_empty = [0] * len(classes)\n",
    "\n",
    "# training set, bag of words for each sentence\n",
    "for doc in documents:\n",
    "    # initialize our bag of words\n",
    "    bag = []\n",
    "    # list of tokenized words for the pattern\n",
    "    pattern_words = doc[0]\n",
    "    # stem each word\n",
    "    pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]\n",
    "    # create our bag of words array\n",
    "    for w in words:\n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\n",
    "\n",
    "    training.append(bag)\n",
    "    # output is a '0' for each tag and '1' for current tag\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "    output.append(output_row)\n",
    "\n",
    "print (\"# words\", len(words))\n",
    "print (\"# classes\", len(classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sample of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # sample training/output\n",
    "# i = 0\n",
    "# w = documents[i][0]\n",
    "# print ([stemmer.stem(word.lower()) for word in w])\n",
    "# print (training[i])\n",
    "# print (output[i])\n",
    "# documents[0]\n",
    "# len(training)\n",
    "output[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some functions needed to implement nueral network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# compute sigmoid nonlinearity\n",
    "def sigmoid(x):\n",
    "    output = 1/(1+np.exp(-x))\n",
    "    return output\n",
    "\n",
    "# convert output of sigmoid function to its derivative\n",
    "def sigmoid_output_to_derivative(output):\n",
    "    return output*(1-output)\n",
    " \n",
    "def clean_up_sentence(sentence):\n",
    "    # tokenize the pattern\n",
    "    sentence_words = nltk.word_tokenize(sentence)\n",
    "    # stem each word\n",
    "    sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n",
    "    return sentence_words\n",
    "\n",
    "# return bag of words array: 0 or 1 for each word in the bag that exists in the sentence\n",
    "def bow(sentence, words, show_details=False):\n",
    "    # tokenize the pattern\n",
    "    sentence_words = clean_up_sentence(sentence)\n",
    "    # bag of words\n",
    "    bag = [0]*len(words)  \n",
    "    for s in sentence_words:\n",
    "        for i,w in enumerate(words):\n",
    "            if w == s: \n",
    "                bag[i] = 1\n",
    "                if show_details:\n",
    "                    print (\"found in bag: %s\" % w)\n",
    "\n",
    "    return(np.array(bag))\n",
    "\n",
    "def think(sentence, show_details=False):\n",
    "    x = bow(sentence.lower(), words, show_details)\n",
    "    if show_details:\n",
    "        print (\"sentence:\", sentence, \"\\n bow:\", x)\n",
    "    # input layer is our bag of words\n",
    "    l0 = x\n",
    "    # matrix multiplication of input and hidden layer\n",
    "    l1 = sigmoid(np.dot(l0, synapse_0))\n",
    "    # output layer\n",
    "    l2 = sigmoid(np.dot(l1, synapse_1))\n",
    "    return l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to train data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(X, y, hidden_neurons=10, alpha=1, epochs=50000, dropout=False, dropout_percent=0.5):\n",
    "\n",
    "    print (\"Training with %s neurons, alpha:%s, dropout:%s %s\" % (hidden_neurons, str(alpha), dropout, dropout_percent if dropout else '') )\n",
    "    print (\"Input matrix: %sx%s    Output matrix: %sx%s\" % (len(X),len(X[0]),1, len(classes)) )\n",
    "    np.random.seed(1)\n",
    "\n",
    "    last_mean_error = 1\n",
    "    # randomly initialize our weights with mean 0\n",
    "    synapse_0 = 2*np.random.random((len(X[0]), hidden_neurons)) - 1\n",
    "    synapse_1 = 2*np.random.random((hidden_neurons, len(classes))) - 1\n",
    "\n",
    "    prev_synapse_0_weight_update = np.zeros_like(synapse_0)\n",
    "    prev_synapse_1_weight_update = np.zeros_like(synapse_1)\n",
    "\n",
    "    synapse_0_direction_count = np.zeros_like(synapse_0)\n",
    "    synapse_1_direction_count = np.zeros_like(synapse_1)\n",
    "        \n",
    "    for j in iter(range(epochs+1)):\n",
    "\n",
    "        # Feed forward through layers 0, 1, and 2\n",
    "        layer_0 = X\n",
    "        layer_1 = sigmoid(np.dot(layer_0, synapse_0))\n",
    "                \n",
    "        if(dropout):\n",
    "            layer_1 *= np.random.binomial([np.ones((len(X),hidden_neurons))],1-dropout_percent)[0] * (1.0/(1-dropout_percent))\n",
    "\n",
    "        layer_2 = sigmoid(np.dot(layer_1, synapse_1))\n",
    "\n",
    "        # how much did we miss the target value?\n",
    "        layer_2_error = y - layer_2\n",
    "\n",
    "        if (j% 10000) == 0 and j > 5000:\n",
    "            # if this 10k iteration's error is greater than the last iteration, break out\n",
    "            if np.mean(np.abs(layer_2_error)) < last_mean_error:\n",
    "                print (\"delta after \"+str(j)+\" iterations:\" + str(np.mean(np.abs(layer_2_error))) )\n",
    "                last_mean_error = np.mean(np.abs(layer_2_error))\n",
    "            else:\n",
    "                print (\"break:\", np.mean(np.abs(layer_2_error)), \">\", last_mean_error )\n",
    "                break\n",
    "                \n",
    "        # in what direction is the target value?\n",
    "        # were we really sure? if so, don't change too much.\n",
    "        layer_2_delta = layer_2_error * sigmoid_output_to_derivative(layer_2)\n",
    "\n",
    "        # how much did each l1 value contribute to the l2 error (according to the weights)?\n",
    "        layer_1_error = layer_2_delta.dot(synapse_1.T)\n",
    "\n",
    "        # in what direction is the target l1?\n",
    "        # were we really sure? if so, don't change too much.\n",
    "        layer_1_delta = layer_1_error * sigmoid_output_to_derivative(layer_1)\n",
    "        \n",
    "        synapse_1_weight_update = (layer_1.T.dot(layer_2_delta))\n",
    "        synapse_0_weight_update = (layer_0.T.dot(layer_1_delta))\n",
    "        \n",
    "        if(j > 0):\n",
    "            synapse_0_direction_count += np.abs(((synapse_0_weight_update > 0)+0) - ((prev_synapse_0_weight_update > 0) + 0))\n",
    "            synapse_1_direction_count += np.abs(((synapse_1_weight_update > 0)+0) - ((prev_synapse_1_weight_update > 0) + 0))        \n",
    "        \n",
    "        synapse_1 += alpha * synapse_1_weight_update\n",
    "        synapse_0 += alpha * synapse_0_weight_update\n",
    "        \n",
    "        prev_synapse_0_weight_update = synapse_0_weight_update\n",
    "        prev_synapse_1_weight_update = synapse_1_weight_update\n",
    "\n",
    "    now = datetime.datetime.now()\n",
    "\n",
    "    # persist synapses\n",
    "    synapse = {'synapse0': synapse_0.tolist(), 'synapse1': synapse_1.tolist(),\n",
    "               'datetime': now.strftime(\"%Y-%m-%d %H:%M\"),\n",
    "               'words': words,\n",
    "               'classes': classes\n",
    "              }\n",
    "    synapse_file = \"synapses.json\"\n",
    "\n",
    "    with open(synapse_file, 'w') as outfile:\n",
    "        json.dump(synapse, outfile, indent=4, sort_keys=True)\n",
    "    print (\"saved synapses to:\", synapse_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to convert list to array and train main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 10 neurons, alpha:0.1, dropout:False \n",
      "Input matrix: 1594x41468    Output matrix: 1x3\n",
      "delta after 10000 iterations:0.0665105275385\n",
      "delta after 20000 iterations:0.0610711168863\n",
      "delta after 30000 iterations:0.0561908365355\n",
      "delta after 40000 iterations:0.0533465919346\n",
      "delta after 50000 iterations:0.0461560407785\n",
      "saved synapses to: synapses.json\n",
      "processing time: 33060.51151227951 seconds\n"
     ]
    }
   ],
   "source": [
    "X = np.array(training)\n",
    "y = np.array(output)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "train(X, y, hidden_neurons=10, alpha=0.1, epochs=50000, dropout=False, dropout_percent=0.2)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print (\"processing time:\", elapsed_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To predict and classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# probability threshold\n",
    "ERROR_THRESHOLD = 0.2\n",
    "# load our calculated synapse values\n",
    "synapse_file = 'synapses.json' \n",
    "with open(synapse_file) as data_file: \n",
    "    synapse = json.load(data_file) \n",
    "    synapse_0 = np.asarray(synapse['synapse0']) \n",
    "    synapse_1 = np.asarray(synapse['synapse1'])\n",
    "\n",
    "def classify(sentence, show_details=False):\n",
    "    results = think(sentence, show_details)\n",
    "\n",
    "    results = [[i,r] for i,r in enumerate(results) if r>ERROR_THRESHOLD ] \n",
    "    results.sort(key=lambda x: x[1], reverse=True) \n",
    "    return_results =[[classes[r[0]],r[1]] for r in results]\n",
    "    #print (\"\\n classification: %s\" % ( return_results))\n",
    "    return return_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Class_2', 0.50000336646698496],\n",
       " ['Class_3', 0.49998749989800162],\n",
       " ['Class_1', 0.49998655760523392]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify(\"  Rework BGA Reball Made Easy Simple and easy to use ! Datum FG Framed Stencils Fixtures Carriers, Pressfit, Router, Wave, & Custom Ministencils Multilevel (Stepdown) Durostone Electropolish Stencils Stencil Types Flexframe Datum PhD Inspection Overlay PrintPart Fixture for Rework Ideal for replacement of micro devices Printpart Fixtures Knowledge Products Technology Fixtures Support Pin Plate Custom Fixtures Router Prototype EpoCoat Stencils Terms of Use PressFit Connector Dies Precision milled SS clad aluminum dies for pressfit SMT Stencil Thickness UV cured nanocoating Area Ratio Calculator Multilevel Stencils Support Pin Locating Plates (SPin) Easy to use template for locating support pins Pressfit Fixtures Area Ratio Report ScanCAD BGA Reballing Fixtures Contact SMT Carriers Stencil + Fixture Rework ReBalling Fixture, PrintPart, Ministencils & more HAAS Nano Coat Stencils Stencils Framed, Frameless or Proto Mini-Stencils for Printing Solder Paste BGA, connectors, LGA, QFN, etc. Stencils About Us B O O T S L o g i n Submit Dear valued customer Welcome to our new website It will be the new gateway for BOOTS On top right hand side of the page are your login boxes Please use your username and password and the site will take you to BOOTS Note that we are also updating BOOTS to be compatible with the latest server OS and this will be launched shortly Thank you for your cooperation Knowledge Ordering Stencil Types SMT Stencil Thickness Providing Data Why Us Stencil Fixture Info Bank Area Ratio Report Area Ratio Calculator App Info Technology Equipment LPKF G6080 Electropolish Stencils ScanCAD HAAS Services EpoCoat Stencils Electropolish Stencils Nano Coat Stencils Multilevel Stepdown UV cured nanocoating Materials Datum PhD Datum FG Durostone Tension Stencils Products Stencils Framed Stencils Frameless Stencils Prototype Multilevel Stencils Fixtures SMT Carriers Router Pressfit Fixtures Wave Solder Fixtures Custom Fixtures Rework Ministencils BGA Reballing Fixtures Printpart Fixtures Nozzles Other Flexframe Inspection Overlay Support Pin Plate About Us Beam On Technology Corporation was established in October 1992 founded by manufacturing engineers with extensive knowledge and expertise in the assembly process Our founding mission was to provide integrated service products to the SMT assembly industry engineered for ease of use that both increase yields and reduce defects This continues to be our goal Since the introduction of our first revolutionary product \"\" Band Etch Technology(tm) for Stencils \"\" we are constantly introducing new products that respond to changes in technology By working closely with our customers in product development we can go the \"\" extra mile \"\" to meet their needs Our Family of Service Products include Solder Paste Stencils...Our proprietary Band Etch Technology(tm) and laser cut stencils Multi Step stencils Our PrintPart System which is used to print directly on component contacts Rework mini stencils Inspection Template Overlay SPin Plate Support Pin Locator Plate Surface Mount Transport Plates Selective and Non Selective Wave Solder Pallets Press Fit Fixtures Printed Circuit Board Stiffeners Metal Squeegee Blades and Blade Assemblies Ball Grid Array BGA re ball fixtures Box Build Assembly Aids Photo Plotting All of our fixtures and other assembly aids are directly designed from Gerber data assuring accuracy that meets or exceeds all tolerances required for your specific SMT assembly process Beam On Technology boasts not one but two of the best Stencil Laser systems from LPKF With their high Aperture cutting speed combined with state of the art fiber optic cutting technology no other stencil Vendor can service you like we can Everything we make is designed beyond the door meaning our products perform better since they are created for use and not just a commodity for us to sell to people We take pride in what we do so any problems our customers have affect us personally We are a responsible company and treat our customers with respect and confidentiality All data sent to us resides on our own in house servers Our ITAR certification gives you piece of mind that data received by us will be kept confidential and secure We primarily accept Gerber data but can work with most types of files including AutoCAD and ODB Mission Statement It is our business to provide our customer with cost effective products without compromising our commitment to quality We consider our customer our primary concern We strive for reliable on time delivery We dedicate ourselves to perpetual technical innovations and product improvements Stencils Framed Frameless or Proto Fixtures Carriers Pressfit Router Wave Custom Rework ReBalling Fixture PrintPart Ministencils more Technical knowledge Sensibly Applied Home About Us Contact Terms of Use Privacy Policy Restrictions Disclaimers 2014 Beam On Technology Next Day Stencils Free Shipping Beam On offers a variety of family of services and products Stencils Fixtures EpoCoat Flex Frame Rework more EpoCoat Stencils Flex Frame Fixtures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Class_3', 0.97663437888614435]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify(\"  LA36400 Shopping Cart DH361R My Wishlist DE364R DE361R DE362R FVK363R ITAP363 FVK362R AC364RG Sell Your Electrical We Purchase >> Blog BOS14351 FA36100 Disconnect ADS36200HDFP DH362R ITAP362 Log In FVK361R My Account DE365R BOS14353 BOS14352 DE363R Close FA36030 Send Request Circuit Breakers AC365RG PPE Equipment Download Company Brochure >> BOS14355 ITAP361 Switchboards Help KA36200 About Us JavaScript seems to be disabled in your browser You must have JavaScript enabled in your browser to utilize the functionality of this website Help Shopping Cart 0 00 You have no items in your shopping cart My Account My Wishlist My Cart My Quote Log In BD Electrical Worldwide Supply Remanufacturing the past SUSTAINING THE FUTURE Hours and Location Michigan Howell 8 5 EST 800 548 7904 Home Bus Duct Bus Plugs Switchboards Circuit Breakers PPE Equipment Transformers Disconnect Bus Duct Bus Plugs Switchboards Circuit Breakers PPE Equipment Transformers Disconnect Home >> About Us About Us BD Electrical established in 1992 supplies a complete array of used and surplus electrical distribution equipment Today specializing in used and surplus bus duct bus plugs we offer one of the most complete inventories of O E M equipment such as Square D Siemens Westinghouse and General Electric products We also carry several other types of equipment including transformers switchgear circuit breakers panel boards safety disconnects and fuses Our reconditioning process is safety oriented and second to no one It includes performance testing which meets or exceeds O E M standards backed by our two year limited warranty Our goal is simple Customer satisfaction through repeat business We ve earned our reputation with reliable competitively priced products and fast courteous service Give us a call and experience first hand that commitment to quality Browse Bus Duct Bus Plugs Switchboards Circuit Breakers PPE Equipment Transformers Disconnect Navigation Home About Us Contact Us Blog Contact Information Michigan Howell 8 5 EST 800 548 7904 Products ABD408 4 BDP304 BDP306 CP2308G ADS36200HDFP FA36030 FA36100 KA36200 LA36400 AC363RG AC364RG AC365RG BOS14351 BOS14352 BOS14353 BOS14354 BOS14355 DE361R DE362R DE363R DE364R DE365R DH361R DH362R DH363R FVK361R FVK362R FVK363R FVK364R FVK365RT ITAP361 ITAP362 ITAP363 (c) 2013 BD Electrical All rights reserved video title video content BD Electrical established in 1992 supplies a complete array of used refurbished and surplus electrical distribution equipment Call 800 548 7904 Used Busway Siemens ITE Breakers Square D Bus Plugs Panel Boards Transformers bd electrical bus plugs bus duct used surplus equipment westinghouse ge electric circuit breaker switchgear ite panel boards bd electrical michigan bd electrical worldwide supply electrical supply bd electrical howell\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Class_2', 0.99859959506883467]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify(\"  Products & Solutions University Sponsorships Defense White Papers MiniMax Find a local contact Disposable Marine & Underwater   Twitter http://www.sensor-test.de Medical AluLite High-speed data: MiniMax with Ethernet and 20m/24h IP68 Pharma Railway News Brass   Oil & Gas Products Fischer Rugged Flash Drive SolarStratos Terms of Use Imprint LP360   Facebook Nuclear Food Catalogues Fischer Core Series News & Events Accessories Pharmaceutical People New microsite, video and brochure on Fischer Connectors' medical connectivity solutions Extreme Energy Videos World Map & Locations Military website Subscribe now Contact Us Electronica 2016 Fischer MiniMax(tm) Series Medical website Cookies Policy Tools Twiice Ask our engineers Events Fischer Freedom(tm) Series Breakthrough connectivity offers EASY mating, cleaning and integration Book an appointment Robonation Balt Military Expo Login Industrial CAD 3D Files About us Skip to main content Login Global Global UK US Switzerland EN Switzerland DE Switzerland FR France Italy Spain LATAM Japan Germany Microsite Military website Medical website Home Products Connectors Overview Fischer Core Series Brass Stainless Steel AluLite Plastic Disposable Broadcast Fischer UltiMate(tm) Series UltiMate Rugged Flash Drive Fischer FiberOptic Series FiberOptic Fischer MiniMax(tm) Series MiniMax Rugged Flash Drive Fischer Freedom(tm) Series LP360 Fischer Rugged Flash Drive Cable Assemblies Custom Solutions Accessories Tools Applications Automotive Broadcast Defense Energy Extreme Food Industrial Instrumentation Marine Underwater Medical Nuclear Oil Gas Pharma Railway Robotics Unmanned Vehicles Security Transportation Vacuum Technical Downloads Hot Topics Catalogues Technical Specifications Technical Drawings User Instructions CAD 3D Files White Papers Infographics Case Studies Videos News Events News Events DSEI 2017 Electronica 2016 Press Room About us History Our Commitment People Products Solutions Planet Quality Certifications Partnerships Sponsorships SolarStratos Twiice Robonation University Sponsorships Jobs Careers Contact us Contact Us World Map Locations Terms of Use Privacy Policy Cookies Policy Sales Terms Conditions Imprint Sitemap About us History Our Commitment People Products Solutions Planet Quality Certifications Partnerships Sponsorships SolarStratos Twiice Robonation University Sponsorships Jobs Careers Fischer Connectors has been designing manufacturing and distributing high performance connectors and cable assembly solutions for 60 years Our connectors are known for their reliability precision and resistance to demanding and harsh environments Fischer Connectors ' products are commonly used in fields requiring faultless quality such as medical equipment industrial instrumentation measuring and testing devices broadcast telecommunication and military forces Primary design and manufacturing facilities are located in Saint Prex Switzerland with eight subsidiaries and many distributors located worldwide History Almost 60 years ago Walter Werner Fischer developed the first sealed push pull connector Since then our company has evolved into a state of the art developer and manufacturer of circular connectors Learn more    Values Our values serve as a compass for our actions to deliver the highest quality standards and answer your needs Learn more   Jobs Careers Fischer Connectors offers job opportunities around the world in modern and dynamic environments where teamwork is essential Learn more    Quality Environment Fischer Connectors is committed to quality and respect of the environment throughout every phase of the company s operations Learn more    Home Products Connectors Overview Fischer Core Series Brass Stainless Steel AluLite Plastic Disposable Broadcast Fischer UltiMate(tm) Series UltiMate Rugged Flash Drive Fischer FiberOptic Series FiberOptic Fischer MiniMax(tm) Series MiniMax Rugged Flash Drive Fischer Freedom(tm) Series LP360 Fischer Rugged Flash Drive Cable Assemblies Custom Solutions Accessories Tools Applications Automotive Broadcast Defense Energy Extreme Food Industrial Instrumentation Marine Underwater Medical Nuclear Oil Gas Pharma Railway Robotics Unmanned Vehicles Security Transportation Vacuum Technical Downloads Hot Topics Catalogues Technical Specifications Technical Drawings User Instructions CAD 3D Files White Papers Infographics Case Studies Videos News Events News Events DSEI 2017 Electronica 2016 Press Room About us History Our Commitment People Products Solutions Planet Quality Certifications Partnerships Sponsorships SolarStratos Twiice Robonation University Sponsorships Jobs Careers Contact us Contact Us World Map Locations Terms of Use Privacy Policy Cookies Policy Sales Terms Conditions Imprint Sitemap Copyright (c) 2018 Fischer Connectors SA All rights reserved Fischer Connectors manufactures high performance push pull circular connectors and cable assemblies Check our range of connectors and cable solutions now\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Class_1', 0.99620297535870017]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify(\"  New Website Testimonial Policies Parts Catalog Contact Support Forum Documentation Themes WordPress Blog Products Spindle Parts Latest News Kennard Parts Suggest Ideas Legal/Disclaimers WordPress Planet News About CDT Home Latest News Testimonial Products Parts Catalog About CDT History Staff Policies Centrum Legal Disclaimers Contact About CDT Custom Drilling Technologies established in 1990 has been providing superior customer service to the printed circuit board industry for almost 20 years We specialize in Excellon Drilling and Routing Equipment Parts and Service Our staff has over sixty years of combined experience in the design building troubleshooting operation programming and process control relating to these machines We offer a wide variety of solutions for practically all manufacturing requirements along with free one on one customer technical support There is no job too big or too small Contact us with your needs we ' ll be there to help Recent Posts New Website Archives December 2011 Categories News Recent Posts New Website Blog Categories News 1 Blogroll Documentation Plugins Suggest Ideas Support Forum Themes WordPress Blog WordPress Planet About Us Custom Drilling Technologies offers superior service in rebuilding Excellon machines on site service calls test and evaluation of parts trouble shooting problems and finding solutions to your needs Detailed information about the services we offer and how Custom Drilling Technologies can help you are below (c) 2008 Custom Drilling \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some more examples: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv('C:/Users/trainees/Desktop/AllData/data/Classificationtabletest.csv')\n",
    "data = data[pd.notnull(data['tokenized_source'])]\n",
    "data=data[data.Category != 'None']\n",
    "#data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.aberdeentech.com [['Class_3', 0.50016357346215368], ['Class_2', 0.49982987279985142], ['Class_1', 0.49898389361852757]]\n",
      "https://www.ablesaw.com [['Class_3', 0.97653278943106148]]\n",
      "https://www.abl-services.com [['Class_3', 0.9766328173943748]]\n",
      "https://www.aesscales.com [['Class_2', 0.99860120391707707]]\n",
      "https://www.aidexprecision.com [['Class_2', 0.99859344381241755]]\n",
      "https://www.eeckstrand@alertus.com [['Class_3', 0.97663389012305102]]\n",
      "https://www.allamericanfire.com [['Class_3', 0.97661091443127901]]\n",
      "https://www.sburwell@allwestunderground.com [['Class_3', 0.97663419414694086]]\n",
      "https://www.altonrental.com [['Class_3', 0.97663438200875075]]\n",
      "https://www.altramotion.com [['Class_3', 0.97661619280407042]]\n",
      "https://www.casto@americanbroach.com [['Class_3', 0.97662882775480775]]\n",
      "https://www.american-carbide.com [['Class_3', 0.97663438419469983]]\n",
      "https://www.ami-mfg.com [['Class_2', 0.99787488058737184]]\n"
     ]
    }
   ],
   "source": [
    "for index,row in data.iterrows():\n",
    "    x1=classify(row[\"tokenized_source\"])\n",
    "    print(row[\"url\"],x1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
