{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Sraping Data For Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "All imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "import bs4\n",
    "import html2text\n",
    "import sys\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from urllib.parse import urlparse\n",
    "from unidecode import unidecode\n",
    "from copyrightextractor import htmlextractor\n",
    "import sys\n",
    "import os\n",
    "import pyap\n",
    "from html_to_etree import parse_html_bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Initialization of different arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "h = html2text.HTML2Text()\n",
    "h.ignore_links = True\n",
    "exclude = set(string.punctuation)\n",
    "about_us_words=['About us','about','about us', 'About','about_us','about-us','About-us','About-us']\n",
    "contact_us_words=['Contact','contact','Contact Us','Contact us','contact','contact us', 'contact','contact_us','contact-us','Contact-us','Contact_us']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "to disable prints for some functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class HiddenPrints:\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        sys.stdout = None\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout = self._original_stdout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "To remove non ascii elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def remove_non_ascii(text):\n",
    "    return unidecode(str(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "To select only visible tags in document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, bs4.element.Comment):\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "to extract whole text from html used in finding information in about us page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def text_from_html(body):\n",
    "    soup = bs4.BeautifulSoup(body, 'html.parser')\n",
    "    texts = soup.findAll(text=True)\n",
    "    visible_texts = filter(tag_visible, texts)  \n",
    "    return u\" \".join(t.strip() for t in visible_texts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "to extract meta tag from html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def meta_from_html(body1):\n",
    "    str1=''\n",
    "    soup1 = bs4.BeautifulSoup(body1, 'html.parser')\n",
    "    for tag in soup1.find_all('meta'):\n",
    "        if 'name' in tag.attrs.keys() and tag.attrs['name'].strip().lower() in ['description', 'keywords']:\n",
    "            str1=str1+tag.attrs['content']+\" \"\n",
    "    return str1    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "to remove punctuation from text so wont hinder in tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def remove_punc(text1):\n",
    "    z=''\n",
    "    for ch in text1:\n",
    "        if ch in exclude:\n",
    "            z+=' '\n",
    "        else:\n",
    "            z=z+ch\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "to create a dictionary of anchor tags i.e. its navigation name and link used for finding about us link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def a_dict_from_html(body2):\n",
    "    a_dict={}\n",
    "    soup2 = bs4.BeautifulSoup(body2, 'html.parser')\n",
    "    try:\n",
    "        for tag in soup2.find_all('a', href=True):\n",
    "            a_dict[str(tag.get_text().strip())]=str(tag['href'])    \n",
    "    except:\n",
    "        return a_dict \n",
    "    else:\n",
    "        return a_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "to validate the about us link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def uri_validator1(x):\n",
    "    try:\n",
    "        result = urlparse(x)\n",
    "        if result.scheme!=\"\" or result.netloc!=\"\" or result.path[0]==\"/\" or result.path[-3:]==\"php\" or result.path[-4:]==\"html\" or result.path[-3:]==\"asp\":\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        #return result.scheme and result.netloc and result.path\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "to convert relative to absolute address of about us link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def url_corrector(url3,relative):\n",
    "    x=urlparse(relative)\n",
    "    newurl=\"\"\n",
    "    if x.scheme==\"\" or x.netloc==\"\":\n",
    "        if relative[0]==\"/\":\n",
    "            newurl=url3+relative\n",
    "        else :\n",
    "            newurl=url3+\"/\"+relative\n",
    "    else:\n",
    "        newurl=relative\n",
    "    return newurl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "to extract about us data from link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def aboutusscraper(About_us_link1,url4):\n",
    "    try:\n",
    "        page1 = requests.get(About_us_link1)\n",
    "        try:\n",
    "            x=text_from_html(page1.content)\n",
    "        except:\n",
    "            x=h.handle(page1.content)\n",
    "            x=x.replace(\"*\",\" \")\n",
    "            x=x.replace(\"\\n\",\" \")\n",
    "        x=remove_punc(x)\n",
    "        list_of_words=word_tokenize(x)\n",
    "        text1=' '.join(list_of_words)\n",
    "        text1=\" \"+text1\n",
    "        text1=remove_non_ascii(text1)\n",
    "        return text1\n",
    "    except:\n",
    "        print(\"About_us_link Broken for \",About_us_link1,\" for website: \",url4)\n",
    "        return \" \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "to extract contact us data from link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def contactusscraper(contact_us_link2,url5):\n",
    "    list_of_link_dummy=['','','','','','','']\n",
    "    list_of_address_dummy=['','','']\n",
    "    list_of_phone_dummy=['','','']\n",
    "    list_of_email_dummy=['','','']\n",
    "    ctr_1=0\n",
    "    ctr_2=0\n",
    "    ctr_3=0\n",
    "    regex_email=re.compile(r'''(?:[a-z0-9!#$%&'*+/=?^_`{|}~-]+(?:\\.[a-z0-9!#$%&'*+/=?^_`{|}~-]+)*|\"(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21\\x23-\\x5b\\x5d-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])*\")@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?|\\[(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?|[a-z0-9-]*[a-z0-9]:(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21-\\x5a\\x53-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])+)\\])''')\n",
    "    phonePattern = re.compile(r'''(\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{3}\\)\\s*\\d{3}[-\\.\\s]??\\d{4}|\\d{3}[-\\.\\s]??\\d{4})''', re.VERBOSE)\n",
    "    try:\n",
    "        page1 = requests.get(contact_us_link2)\n",
    "        try:\n",
    "            x=text_from_html(page1.content)\n",
    "        except:\n",
    "            x=h.handle(page1.content)\n",
    "            x=x.replace(\"*\",\" \")\n",
    "            x=x.replace(\"\\n\",\" \")\n",
    "        text_for_email=remove_non_ascii(x)\n",
    "        x=remove_punc(x)\n",
    "        list_of_words=word_tokenize(x)\n",
    "        text1=' '.join(list_of_words)\n",
    "        text1=\" \"+text1\n",
    "        list_of_link_dummy=get_social_links(page1.content,page1.headers.get('content-type'))\n",
    "        text1=remove_non_ascii(text1)\n",
    "        phonenumbers=phonePattern.findall(str(text1))\n",
    "        for phn in  phonenumbers:\n",
    "            list_of_phone_dummy[ctr_1]=str(phn)\n",
    "            ctr_1+=1\n",
    "            if ctr_1 > 2:\n",
    "                break\n",
    "        addresses = pyap.parse(str(text1),country='US')\n",
    "        addresses+=pyap.parse(str(text1), country='CA')\n",
    "        for address in addresses:\n",
    "            list_of_address_dummy[ctr_2]=str(address)\n",
    "            ctr_2+=1\n",
    "            if ctr_2 > 2:\n",
    "                break\n",
    "        emails=re.findall(regex_email,text_for_email)\n",
    "        for email in emails:\n",
    "            #print(email)\n",
    "            list_of_email_dummy[ctr_3]=str(email)\n",
    "            ctr_3+=1\n",
    "            if ctr_3 > 2:\n",
    "                break\n",
    "        return text1, list_of_address_dummy, list_of_phone_dummy, list_of_email_dummy, list_of_link_dummy\n",
    "    except Exception as e:\n",
    "        #print(e)\n",
    "        print(\"Contact_us_link Broken for \",contact_us_link2,\" for website: \",url5)\n",
    "        return \"\", list_of_address_dummy, list_of_phone_dummy, list_of_email_dummy, list_of_link_dummy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "to save elements of dict of Anchor tags into a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def saveinstring(dict1):\n",
    "    string2=''\n",
    "    for tag_text,tag_link in dict1.items():\n",
    "        string2=string2+\" \"+tag_text\n",
    "    return string2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "to find about us link from navigation text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def findaboutuslink(dict1,url2):\n",
    "    About_us_link1=''\n",
    "    for tag_text,tag_link in dict1.items():\n",
    "        if any(word in tag_text for word in about_us_words):\n",
    "            if uri_validator1(str(tag_link))==True:\n",
    "                About_us_link1=url_corrector(url2,tag_link)\n",
    "                break\n",
    "    return About_us_link1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "to find contact us link from navigation text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def findcontactuslink(dict1,url1):\n",
    "    Contact_us_link1=''\n",
    "    for tag_text,tag_link in dict1.items():\n",
    "        #print(tag_text)\n",
    "        if any(word in tag_text for word in contact_us_words):\n",
    "            if uri_validator1(str(tag_link))==True:\n",
    "                Contact_us_link1=url_corrector(url1,tag_link)\n",
    "                break\n",
    "    #print(Contact_us_link1)\n",
    "    return Contact_us_link1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find social links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "import re\n",
    "\n",
    "import six\n",
    "PREFIX = r'https?://(?:www\\.)?'\n",
    "SITES = ['twitter.com/', 'youtube.com/',\n",
    "         '(?:[a-z]{2}\\.)?linkedin.com/',\n",
    "         'github.com/', '(?:[a-z]{2}-[a-z]{2}\\.)?facebook.com/', 'fb.co',\n",
    "         'plus\\.google.com/', 'pinterest.com/', 'instagram.com/',\n",
    "         'snapchat.com/', 'flipboard.com/', 'flickr.com',\n",
    "         'google.com/+', 'weibo.com/', 'periscope.tv/',\n",
    "         'telegram.me/', 'soundcloud.com', 'feeds.feedburner.com',\n",
    "         'vimeo.com', 'slideshare.net', 'vkontakte.ru']\n",
    "BETWEEN = ['user/', 'add/', 'pages/', '#!/', 'photos/',\n",
    "           'u/0/']\n",
    "ACCOUNT = r'[\\w\\+_@\\.\\-/%]+'\n",
    "PATTERN = (\n",
    "    r'%s(?:%s)(?:%s)?%s' %\n",
    "    (PREFIX, '|'.join(SITES), '|'.join(BETWEEN), ACCOUNT))\n",
    "SOCIAL_REX = re.compile(PATTERN, flags=re.I)\n",
    "BLACKLIST_RE = re.compile(\n",
    "    \"\"\"\n",
    "    sharer.php|\n",
    "    /photos/.*\\d{6,}|\n",
    "    google.com/(?:ads/|\n",
    "                  analytics$|\n",
    "                  chrome$|\n",
    "                  intl/|\n",
    "                  maps/|\n",
    "                  policies/|\n",
    "                  search$\n",
    "               )|\n",
    "    instagram.com/p/|\n",
    "    /share\\?|\n",
    "    /status/|\n",
    "    /hashtag/|\n",
    "    home\\?status=|\n",
    "    twitter.com/intent/|\n",
    "    twitter.com/share|\n",
    "    search\\?|\n",
    "    /search/|\n",
    "    pinterest.com/pin/create/|\n",
    "    vimeo.com/\\d+$|\n",
    "    /watch\\?\"\"\",\n",
    "    flags=re.VERBOSE)\n",
    "def matches_string(string):\n",
    "    \"\"\" check if a given string matches known social media url patterns \"\"\"\n",
    "    return SOCIAL_REX.match(string) and not BLACKLIST_RE.search(string)\n",
    "def find_links_tree(tree):\n",
    "    \"\"\"\n",
    "    find social media links/handles given an lxml etree.\n",
    "    TODO:\n",
    "    - `<fb:like href=\"http://www.facebook.com/elDiarioEs\"`\n",
    "    - `<g:plusone href=\"http://widgetsplus.com/\"></g:plusone>`\n",
    "    - <a class=\"reference external\" href=\"https://twitter.com/intent/follow?screen_name=NASA\">\n",
    "    \"\"\"\n",
    "    for link in tree.xpath('//*[@href or @data-href]'):\n",
    "        href = link.get('href') or link.get('data-href')\n",
    "        if (href and\n",
    "                isinstance(href, (six.string_types, six.text_type)) and\n",
    "                matches_string(href)):\n",
    "            yield href\n",
    "\n",
    "    for script in tree.xpath('//script[not(@src)]/text()'):\n",
    "        for match in SOCIAL_REX.findall(script):\n",
    "            if not BLACKLIST_RE.search(match):\n",
    "                yield match\n",
    "\n",
    "    for script in tree.xpath('//meta[contains(@name, \"twitter:\")]'):\n",
    "        name = script.get('name')\n",
    "        if name in ('twitter:site', 'twitter:creator'):\n",
    "            # FIXME: track fact that source is twitter\n",
    "            yield script.get('content')\n",
    "def get_from_url(body,headers_content):  # pragma: no cover\n",
    "    \"\"\" get list of social media links/handles given a url \"\"\"\n",
    "    \n",
    "    tree = parse_html_bytes(body, headers_content)\n",
    "\n",
    "    return set(find_links_tree(tree))\n",
    "def initialize_dict_social(dict1):\n",
    "    dict1[\"facebook_link\"]=''\n",
    "    dict1[\"facebook_id\"]=''\n",
    "    dict1[\"twitter_link\"]=''\n",
    "    dict1[\"twitter_id\"]=''\n",
    "    dict1[\"youtube_link\"]=''\n",
    "    dict1[\"youtube_id\"]=''\n",
    "    dict1[\"linkedin_link\"]=''\n",
    "    return dict1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to find list of social links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_social_links(body,headers_content):\n",
    "    set_of_links=get_from_url(body,headers_content)\n",
    "    list1=list(set_of_links)\n",
    "    strface=\"facebook.com\"\n",
    "    strtwit=\"twitter.com\"\n",
    "    stryou=\".youtube.com\"\n",
    "    strlin=\"linkedin.com\"\n",
    "    dict_soc={}\n",
    "    dict_soc=initialize_dict_social(dict_soc)\n",
    "    for str1 in list1:\n",
    "        if strface in str1:\n",
    "            dict_soc[\"facebook_link\"]=str1\n",
    "            dict_soc[\"facebook_id\"]=str1.split(strface+\"/\",1)[1]\n",
    "            #print(\"facebook : \",str1.split(strface+\"/\",1)[1])\n",
    "        if strtwit in str1:\n",
    "            dict_soc[\"twitter_link\"]=str1\n",
    "            dict_soc[\"twitter_id\"]=str1.split(strtwit+\"/\",1)[1]\n",
    "            #print(\"twitter : \",str1.split(strtwit+\"/\",1)[1])\n",
    "        if stryou in str1:\n",
    "            dict_soc[\"youtube_link\"]=str1\n",
    "            dict_soc[\"youtube_link\"]=str1.split(stryou+\"/\",1)[1]\n",
    "            #print(\"youtube : \",str1.split(stryou+\"/\",1)[1])\n",
    "        if strlin in str1:\n",
    "            dict_soc[\"linkedin_link\"]=str1\n",
    "            #print(\"linkedin : \",str1.split(strlin+\"/\",1)[1])\n",
    "    listnew=[dict_soc[\"facebook_link\"],dict_soc[\"facebook_id\"],dict_soc[\"twitter_link\"],dict_soc[\"twitter_id\"],dict_soc[\"youtube_link\"],dict_soc[\"youtube_link\"],dict_soc[\"linkedin_link\"]]\n",
    "    return listnew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Main funtion to scrape and save elements into files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def ScrapeAndSave(url,\n",
    "                  SrNo,\n",
    "                  id_for_table_1,\n",
    "                  newFileWriter1,\n",
    "                  newFileWritercopyright1,\n",
    "                  newFileWritercontact1,\n",
    "                  newFileWriterabout1,\n",
    "                  newFileWriterfinal1):\n",
    "    url=\"https://www.\"+url\n",
    "    final_data_array=[id_for_table_1,SrNo,url]\n",
    "    final_file_data_array=[id_for_table_1,SrNo,url]\n",
    "    copyright_data_array=[id_for_table_1,SrNo,url]\n",
    "    contactus_data_array=[id_for_table_1,SrNo,url]\n",
    "    aboutus_data_array=[id_for_table_1,SrNo,url]\n",
    "    list_of_links=[]\n",
    "    try:\n",
    "        page = requests.get(url)\n",
    "        #page.content=remove_non_ascii(page.content)\n",
    "        About_us_link='' \n",
    "        string1=''\n",
    "        copyright=''\n",
    "        abouttext=''\n",
    "        contacttext=''\n",
    "        Contact_us_link=''\n",
    "        with HiddenPrints():\n",
    "            copyright = htmlextractor.extract(page.content)\n",
    "            copyright=remove_non_ascii(copyright)\n",
    "        copyright_data_array.append(copyright)\n",
    "        try:\n",
    "            dict_a=a_dict_from_html(page.content)\n",
    "#             for tag_text,tag_link in dict_a.items():\n",
    "#                 string1=string1+\" \"+tag_text\n",
    "            string1=string1+\" \"+saveinstring(dict_a)\n",
    "#                 if any(word in tag_text for word in about_us_words):\n",
    "#                     if uri_validator1(str(tag_link))==True:\n",
    "#                         About_us_link=url_corrector(url,tag_link)\n",
    "#                         break\n",
    "            About_us_link=findaboutuslink(dict_a,url)\n",
    "            Contact_us_link=findcontactuslink(dict_a,url)\n",
    "            #print(string1)\n",
    "            if About_us_link!='':\n",
    "                abouttext=aboutusscraper(About_us_link,url)\n",
    "                string1=string1+abouttext\n",
    "            if Contact_us_link!='':\n",
    "                contacttext,address_,phone_,email_,link_=contactusscraper(Contact_us_link,url)\n",
    "                final_file_data_array=final_file_data_array+address_+phone_+email_+link_\n",
    "            aboutus_data_array.append(abouttext)\n",
    "            contactus_data_array.append(contacttext)\n",
    "            \n",
    "        except Exception as e1:\n",
    "            #final_data_array.append(string1)\n",
    "            print(\"Error in trying AboutUs or ContactUs info\")\n",
    "            \n",
    "        try:\n",
    "            x=meta_from_html(page.content)\n",
    "        except:\n",
    "            final_data_array.append(string1)\n",
    "        else:\n",
    "            x=remove_punc(x)\n",
    "            list_of_words=word_tokenize(x)\n",
    "            text=' '.join(list_of_words) \n",
    "            string1=string1+\" \"+text\n",
    "            string1=remove_non_ascii(string1)\n",
    "            final_data_array.append(string1)\n",
    "           \n",
    "    except Exception as e:\n",
    "        print(\"Connection refused for Sr. No : \"+str(SrNo))\n",
    "        newFileWritercopyright1.writerow(copyright_data_array)\n",
    "        newFileWriter1.writerow(final_data_array)\n",
    "        newFileWritercontact1.writerow(contactus_data_array)\n",
    "        newFileWriterabout1.writerow(aboutus_data_array)\n",
    "        newFileWriterfinal1.writerow(final_file_data_array)\n",
    "        #print(e)\n",
    "        return 0\n",
    "    else:\n",
    "        print(\"Done for Sr. No : \"+str(SrNo))\n",
    "        newFileWritercopyright1.writerow(copyright_data_array)\n",
    "        newFileWriter1.writerow(final_data_array)\n",
    "        newFileWritercontact1.writerow(contactus_data_array)\n",
    "        newFileWriterabout1.writerow(aboutus_data_array)\n",
    "        newFileWriterfinal1.writerow(final_file_data_array)\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "selects url from file with its id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "urls=[]\n",
    "IDs=[]\n",
    "countforheader=0\n",
    "with open('C:/Users/trainees/Desktop/urls.csv','r') as userFile:\n",
    "    userFileReader = csv.reader(userFile)\n",
    "    for row in userFileReader:\n",
    "        countforheader+=1\n",
    "        if countforheader==1:\n",
    "            continue\n",
    "        IDs.append(row[0])\n",
    "        urls.append(row[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "loop to scrape data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done for Sr. No : 1\n",
      "Done for Sr. No : 2\n",
      "Done for Sr. No : 3\n",
      "Connection refused for Sr. No : 4\n",
      "Connection refused for Sr. No : 5\n",
      "Done for Sr. No : 6\n",
      "Done for Sr. No : 7\n",
      "Connection refused for Sr. No : 8\n",
      "Connection refused for Sr. No : 9\n",
      "Done for Sr. No : 10\n",
      "Done for Sr. No : 11\n",
      "Done for Sr. No : 12\n",
      "Connection refused for Sr. No : 13\n",
      "Connection refused for Sr. No : 14\n",
      "Done for Sr. No : 15\n",
      "Done for Sr. No : 16\n",
      "Done for Sr. No : 17\n",
      "Done for Sr. No : 18\n",
      "Connection refused for Sr. No : 19\n",
      "Done for Sr. No : 20\n",
      "Connection refused for Sr. No : 21\n",
      "Connection refused for Sr. No : 22\n",
      "Done for Sr. No : 23\n",
      "Connection refused for Sr. No : 24\n",
      "Connection refused for Sr. No : 25\n",
      "Connection refused for Sr. No : 26\n",
      "Connection refused for Sr. No : 27\n",
      "Connection refused for Sr. No : 28\n",
      "Connection refused for Sr. No : 29\n",
      "Done for Sr. No : 30\n"
     ]
    }
   ],
   "source": [
    "z=0\n",
    "folder_path='C:/Users/trainees/Desktop/AllData/data1/'\n",
    "id_for_table=0\n",
    "Table_for_tokenization='Tokenizationtable.csv'\n",
    "Table_for_copyright='Copyright.csv'\n",
    "Table_for_contact_us='ContactUs.csv'\n",
    "Table_for_about_us='AboutUs.csv'\n",
    "Table_for_finalfile='FinalFile.csv'\n",
    "with open(folder_path + Table_for_tokenization , 'a',newline='') as newFile:\n",
    "    newFileWriterMain = csv.writer(newFile,delimiter=\",\")\n",
    "    with open(folder_path + Table_for_copyright , 'a',newline='') as newFileCopy:\n",
    "        newFileWritercopyright = csv.writer(newFileCopy,delimiter=\",\")\n",
    "        with open(folder_path + Table_for_contact_us , 'a',newline='') as newFilecontact:\n",
    "            newFileWritercontact = csv.writer(newFilecontact,delimiter=\",\")\n",
    "            with open(folder_path + Table_for_about_us , 'a',newline='') as newFileabout:\n",
    "                newFileWriterabout = csv.writer(newFileabout,delimiter=\",\")\n",
    "                with open(folder_path + Table_for_finalfile , 'a',newline='') as newFileFinal:\n",
    "                    newFileWriterfinal = csv.writer(newFileFinal,delimiter=\"|\")\n",
    "                    for i in range(len(urls)):\n",
    "                        id_for_table+=1\n",
    "                        out=ScrapeAndSave(urls[i],IDs[i],id_for_table,newFileWriterMain,newFileWritercopyright,newFileWritercontact,newFileWriterabout,newFileWriterfinal)\n",
    "                        z+=out\n",
    "                        if z==15:\n",
    "                            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
